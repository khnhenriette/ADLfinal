{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Plot average causal effects & Frozen-MLP causal tracing"
      ],
      "metadata": {
        "id": "T8e9xEt6ECIX"
      },
      "id": "T8e9xEt6ECIX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we run the causal tracing program, safe the results and plot the average causal effects for better visualization of the effects as well as the causal effects with isolated / frozen MLP modules\n",
        "\n",
        "Model here is: gpt2-medium\n",
        "Average over the forst 100 factual associations in the dataset knowns instead of all 1209 to safe some processing time"
      ],
      "metadata": {
        "id": "ms1SatZ_ENpN"
      },
      "id": "ms1SatZ_ENpN"
    },
    {
      "cell_type": "markdown",
      "id": "f379178d",
      "metadata": {
        "id": "f379178d"
      },
      "source": [
        "# Plot average causal effects\n",
        "\n",
        "This script loads sets of hundreds of causal traces that have been computed by the\n",
        "`experiment.causal_trace` program, and then aggregates the results to compute\n",
        "Average Indirect Effects and Average Total Effects as well as some other information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylneHtzd1cs0"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "cd /content && rm -rf /content/rome\n",
        "git clone --branch CausalTracingTrials --single-branch https://github.com/khnhenriette/ProjectADL rome > install.log 2>&1\n",
        "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
        "pip install --upgrade google-cloud-storage >> install.log 2>&1"
      ],
      "id": "ylneHtzd1cs0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2z43Qsf1cs3"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = False\n",
        "try:\n",
        "    import google.colab, torch, os\n",
        "\n",
        "    IS_COLAB = True\n",
        "    os.chdir(\"/content/rome\")\n",
        "    if not torch.cuda.is_available():\n",
        "        raise Exception(\"Change runtime type to include a GPU.\")\n",
        "except ModuleNotFoundError as _:\n",
        "    pass"
      ],
      "id": "j2z43Qsf1cs3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy_A9l553x1h",
        "outputId": "15e64d65-dee9-4a8d-db17-0fbd5d8175a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ],
      "id": "qy_A9l553x1h"
    },
    {
      "cell_type": "code",
      "source": [
        "# run the causal tracing on 100 factual associtations on gpt medium\n",
        "\n",
        "!python causal_trace.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGa-T-NtfxpP",
        "outputId": "b3a24cf8-ae60-4e9a-bd10-1682b30710ec"
      },
      "id": "uGa-T-NtfxpP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 86.9kB/s]\n",
            "config.json: 100% 689/689 [00:00<00:00, 3.84MB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:01<00:00, 944kB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 917kB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 2.82MB/s]\n",
            "2024-11-21 16:27:51.945172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-21 16:27:51.963638: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-21 16:27:51.969490: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-21 16:27:51.983675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-21 16:27:53.222223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "model.safetensors: 100% 6.43G/6.43G [00:37<00:00, 172MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 749kB/s]\n",
            "data/known_1000.json does not exist. Downloading from https://rome.baulab.info/data/dsets/known_1000.json\n",
            "100% 335k/335k [00:00<00:00, 396kB/s]\n",
            "Loaded dataset with 1209 elements\n",
            "Using noise_level 0.13462981581687927 to match model times 3.0\n",
            "  0% 2/1209 [04:35<46:07:24, 137.57s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/rome/causal_trace.py\", line 751, in <module>\n",
            "    main()\n",
            "  File \"/content/rome/causal_trace.py\", line 105, in main\n",
            "    result = calculate_hidden_flow(\n",
            "  File \"/content/rome/causal_trace.py\", line 329, in calculate_hidden_flow\n",
            "    differences = trace_important_states(\n",
            "  File \"/content/rome/causal_trace.py\", line 388, in trace_important_states\n",
            "    r = trace_with_patch(\n",
            "  File \"/content/rome/causal_trace.py\", line 223, in trace_with_patch\n",
            "    if trace_layers is not None:\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the results of the causal tracing in memory we can access them and plot the averaging figures to get a good overview of the average indirect effects of individual model components over a sample of 100 factual statements."
      ],
      "metadata": {
        "id": "gcxkVW8-36aM"
      },
      "id": "gcxkVW8-36aM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26bba71c",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "26bba71c",
        "outputId": "a6ecf2a5-be57-49eb-da6d-25edc9cd6997"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6503c17d288d>\u001b[0m in \u001b[0;36m<cell line: 178>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mlp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_knowlege\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     what = {\n",
            "\u001b[0;32m<ipython-input-1-6503c17d288d>\u001b[0m in \u001b[0;36mread_knowlege\u001b[0;34m(count, kind, arch)\u001b[0m\n\u001b[1;32m     83\u001b[0m     result = numpy.stack(\n\u001b[1;32m     84\u001b[0m         [\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mavg_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mavg_ee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mavg_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6503c17d288d>\u001b[0m in \u001b[0;36mavg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        }
      ],
      "source": [
        "import numpy, os\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
        "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
        "\n",
        "# Uncomment the architecture to plot.\n",
        "arch = \"gpt2-medium\"\n",
        "archname = \"GPT-2-Medium\"\n",
        "\n",
        "# arch = \"gpt2-xl\"\n",
        "# archname = \"GPT-2-XL\"\n",
        "\n",
        "# arch = \"EleutherAI_gpt-j-6B\"\n",
        "# archname = \"GPT-J-6B\n",
        "\n",
        "# arch = 'EleutherAI_gpt-j-6B'\n",
        "# archname = 'GPT-J-6B'\n",
        "\n",
        "# arch = 'EleutherAI_gpt-neox-20b'\n",
        "# archname = 'GPT-NeoX-20B'\n",
        "\n",
        "\n",
        "class Avg:\n",
        "    def __init__(self):\n",
        "        self.d = []\n",
        "\n",
        "    def add(self, v):\n",
        "        self.d.append(v[None])\n",
        "\n",
        "    def add_all(self, vv):\n",
        "        self.d.append(vv)\n",
        "\n",
        "    def avg(self):\n",
        "        return numpy.concatenate(self.d).mean(axis=0)\n",
        "\n",
        "    def std(self):\n",
        "        return numpy.concatenate(self.d).std(axis=0)\n",
        "\n",
        "    def size(self):\n",
        "        return sum(datum.shape[0] for datum in self.d)\n",
        "\n",
        "\n",
        "def read_knowlege(count=150, kind=None, arch=\"gpt2-medium\"):\n",
        "    dirname = f\"results/{arch}/causal_trace/cases/\"\n",
        "    kindcode = \"\" if not kind else f\"_{kind}\"\n",
        "    (\n",
        "        avg_fe,\n",
        "        avg_ee,\n",
        "        avg_le,\n",
        "        avg_fa,\n",
        "        avg_ea,\n",
        "        avg_la,\n",
        "        avg_hs,\n",
        "        avg_ls,\n",
        "        avg_fs,\n",
        "        avg_fle,\n",
        "        avg_fla,\n",
        "    ) = [Avg() for _ in range(11)]\n",
        "    for i in range(count):\n",
        "        try:\n",
        "            data = numpy.load(f\"{dirname}/knowledge_{i}{kindcode}.npz\")\n",
        "        except:\n",
        "            continue\n",
        "        # Only consider cases where the model begins with the correct prediction\n",
        "        if \"correct_prediction\" in data and not data[\"correct_prediction\"]:\n",
        "            continue\n",
        "        scores = data[\"scores\"]\n",
        "        first_e, first_a = data[\"subject_range\"]\n",
        "        last_e = first_a - 1\n",
        "        last_a = len(scores) - 1\n",
        "        # original prediction\n",
        "        avg_hs.add(data[\"high_score\"])\n",
        "        # prediction after subject is corrupted\n",
        "        avg_ls.add(data[\"low_score\"])\n",
        "        avg_fs.add(scores.max())\n",
        "        # some maximum computations\n",
        "        avg_fle.add(scores[last_e].max())\n",
        "        avg_fla.add(scores[last_a].max())\n",
        "        # First subject middle, last subjet.\n",
        "        avg_fe.add(scores[first_e])\n",
        "        avg_ee.add_all(scores[first_e + 1 : last_e])\n",
        "        avg_le.add(scores[last_e])\n",
        "        # First after, middle after, last after\n",
        "        avg_fa.add(scores[first_a])\n",
        "        avg_ea.add_all(scores[first_a + 1 : last_a])\n",
        "        avg_la.add(scores[last_a])\n",
        "\n",
        "    result = numpy.stack(\n",
        "        [\n",
        "            avg_fe.avg(),\n",
        "            avg_ee.avg(),\n",
        "            avg_le.avg(),\n",
        "            avg_fa.avg(),\n",
        "            avg_ea.avg(),\n",
        "            avg_la.avg(),\n",
        "        ]\n",
        "    )\n",
        "    result_std = numpy.stack(\n",
        "        [\n",
        "            avg_fe.std(),\n",
        "            avg_ee.std(),\n",
        "            avg_le.std(),\n",
        "            avg_fa.std(),\n",
        "            avg_ea.std(),\n",
        "            avg_la.std(),\n",
        "        ]\n",
        "    )\n",
        "    print(\"Average Total Effect\", avg_hs.avg() - avg_ls.avg())\n",
        "    print(\n",
        "        \"Best average indirect effect on last subject\",\n",
        "        avg_le.avg().max() - avg_ls.avg(),\n",
        "    )\n",
        "    print(\n",
        "        \"Best average indirect effect on last token\", avg_la.avg().max() - avg_ls.avg()\n",
        "    )\n",
        "    print(\"Average best-fixed score\", avg_fs.avg())\n",
        "    print(\"Average best-fixed on last subject token score\", avg_fle.avg())\n",
        "    print(\"Average best-fixed on last word score\", avg_fla.avg())\n",
        "    print(\"Argmax at last subject token\", numpy.argmax(avg_le.avg()))\n",
        "    print(\"Max at last subject token\", numpy.max(avg_le.avg()))\n",
        "    print(\"Argmax at last prompt token\", numpy.argmax(avg_la.avg()))\n",
        "    print(\"Max at last prompt token\", numpy.max(avg_la.avg()))\n",
        "    return dict(\n",
        "        low_score=avg_ls.avg(), result=result, result_std=result_std, size=avg_fe.size()\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_array(\n",
        "    differences,\n",
        "    kind=None,\n",
        "    savepdf=None,\n",
        "    title=None,\n",
        "    low_score=None,\n",
        "    high_score=None,\n",
        "    archname=\"GPT2-Medium\",\n",
        "):\n",
        "    if low_score is None:\n",
        "        low_score = differences.min()\n",
        "    if high_score is None:\n",
        "        high_score = differences.max()\n",
        "    answer = \"AIE\"\n",
        "    labels = [\n",
        "        \"First subject token\",\n",
        "        \"Middle subject tokens\",\n",
        "        \"Last subject token\",\n",
        "        \"First subsequent token\",\n",
        "        \"Further tokens\",\n",
        "        \"Last token\",\n",
        "    ]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(3.5, 2), dpi=200)\n",
        "    h = ax.pcolor(\n",
        "        differences,\n",
        "        cmap={None: \"Purples\", \"mlp\": \"Greens\", \"attn\": \"Reds\"}[kind],\n",
        "        vmin=low_score,\n",
        "        vmax=high_score,\n",
        "    )\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_yticks([0.5 + i for i in range(len(differences))])\n",
        "    ax.set_xticks([0.5 + i for i in range(0, differences.shape[1] - 6, 5)])\n",
        "    ax.set_xticklabels(list(range(0, differences.shape[1] - 6, 5)))\n",
        "    ax.set_yticklabels(labels)\n",
        "    if kind is None:\n",
        "        ax.set_xlabel(f\"single patched layer within {archname}\")\n",
        "    else:\n",
        "        ax.set_xlabel(f\"center of interval of 10 patched {kind} layers\")\n",
        "    cb = plt.colorbar(h)\n",
        "    # The following should be cb.ax.set_xlabel(answer), but this is broken in matplotlib 3.5.1.\n",
        "    if answer:\n",
        "        cb.ax.set_title(str(answer).strip(), y=-0.16, fontsize=10)\n",
        "\n",
        "    if savepdf:\n",
        "        os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n",
        "        plt.savefig(savepdf, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "# there are 1209 factual associations in the knowns dataset, i.e. numbered from 0 to 1208\n",
        "the_count = 1208\n",
        "high_score = None  # Scale all plots according to the y axis of the first plot\n",
        "\n",
        "for kind in [None, \"mlp\", \"attn\"]:\n",
        "    d = read_knowlege(the_count, kind, arch)\n",
        "    count = d[\"size\"]\n",
        "    what = {\n",
        "        None: \"Indirect Effect of $h_i^{(l)}$\",\n",
        "        \"mlp\": \"Indirect Effect of MLP\",\n",
        "        \"attn\": \"Indirect Effect of Attn\",\n",
        "    }[kind]\n",
        "    title = f\"Avg {what} over {count} prompts\"\n",
        "    result = numpy.clip(d[\"result\"] - d[\"low_score\"], 0, None)\n",
        "    kindcode = \"\" if kind is None else f\"_{kind}\"\n",
        "    if kind not in [\"mlp\", \"attn\"]:\n",
        "        high_score = result.max()\n",
        "    plot_array(\n",
        "        result,\n",
        "        kind=kind,\n",
        "        title=title,\n",
        "        low_score=0.0,\n",
        "        high_score=high_score,\n",
        "        archname=archname,\n",
        "        savepdf=f\"results/{arch}/causal_trace/summary_pdfs/rollup{kindcode}.pdf\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c896e9ac",
      "metadata": {
        "id": "c896e9ac"
      },
      "source": [
        "## Plot line graph\n",
        "\n",
        "To make confidence intervals visible, we plot the data as line graphs below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe3105",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c1fe3105",
        "outputId": "714e6dcc-45cf-4f73-f184-f2ca933dd848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8da03ff096a2>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m ):\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reading {kind}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_knowlege\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m225\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"low_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6503c17d288d>\u001b[0m in \u001b[0;36mread_knowlege\u001b[0;34m(count, kind, arch)\u001b[0m\n\u001b[1;32m     83\u001b[0m     result = numpy.stack(\n\u001b[1;32m     84\u001b[0m         [\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mavg_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mavg_ee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mavg_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6503c17d288d>\u001b[0m in \u001b[0;36mavg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Times New Roman' not found.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2600x700 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "labels = [\n",
        "    \"First subject token\",\n",
        "    \"Middle subject tokens\",\n",
        "    \"Last subject token\",\n",
        "    \"First subsequent token\",\n",
        "    \"Further tokens\",\n",
        "    \"Last token\",\n",
        "]\n",
        "color_order = [0, 1, 2, 4, 5, 3]\n",
        "x = None\n",
        "\n",
        "cmap = plt.get_cmap(\"tab10\")\n",
        "fig, axes = plt.subplots(1, 3, figsize=(13, 3.5), sharey=True, dpi=200)\n",
        "for j, (kind, title) in enumerate(\n",
        "    [\n",
        "        (None, \"single hidden vector\"),\n",
        "        (\"mlp\", \"run of 10 MLP lookups\"),\n",
        "        (\"attn\", \"run of 10 Attn modules\"),\n",
        "    ]\n",
        "):\n",
        "    print(f\"Reading {kind}\")\n",
        "    d = read_knowlege(225, kind, arch)\n",
        "    for i, label in list(enumerate(labels)):\n",
        "        y = d[\"result\"][i] - d[\"low_score\"]\n",
        "        if x is None:\n",
        "            x = list(range(len(y)))\n",
        "        std = d[\"result_std\"][i]\n",
        "        error = std * 1.96 / math.sqrt(count)\n",
        "        axes[j].fill_between(\n",
        "            x, y - error, y + error, alpha=0.3, color=cmap.colors[color_order[i]]\n",
        "        )\n",
        "        axes[j].plot(x, y, label=label, color=cmap.colors[color_order[i]])\n",
        "\n",
        "    axes[j].set_title(f\"Average indirect effect of a {title}\")\n",
        "    axes[j].set_ylabel(\"Average indirect effect on p(o)\")\n",
        "    axes[j].set_xlabel(f\"Layer number in {archname}\")\n",
        "    # axes[j].set_ylim(0.1, 0.3)\n",
        "axes[1].legend(frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"results/{arch}/causal_trace/summary_pdfs/lineplot-causaltrace.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8203f43f"
      },
      "source": [
        "# Frozen-MLP causal tracing\n",
        "\n",
        "This notebook executes causal traces with all the MLP modules for a token disabled (we also do Attn modules separately), by freezing them at the corrupted state."
      ],
      "id": "8203f43f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f7e67a"
      },
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "id": "b6f7e67a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90ba3338"
      },
      "outputs": [],
      "source": [
        "import os, re\n",
        "import torch, numpy\n",
        "import importlib, copy\n",
        "import transformers\n",
        "from collections import defaultdict\n",
        "from util import nethook\n",
        "from matplotlib import pyplot as plt\n",
        "from experiments.causal_trace import (\n",
        "    ModelAndTokenizer,\n",
        "    make_inputs,\n",
        "    predict_from_input,\n",
        "    decode_tokens,\n",
        "    layername,\n",
        "    find_token_range,\n",
        "    trace_with_patch,\n",
        "    plot_trace_heatmap,\n",
        "    collect_embedding_std,\n",
        ")\n",
        "from util.globals import DATA_DIR\n",
        "from dsets import KnownsDataset"
      ],
      "id": "90ba3338"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c46e43"
      },
      "source": [
        "Load model and compute its corresponding noise level."
      ],
      "id": "16c46e43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ce71fd8"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2-medium\"  # \"gpt2-xl\" or \"EleutherAI/gpt-j-6B\" or \"EleutherAI/gpt-neox-20b\"\n",
        "mt = ModelAndTokenizer(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=IS_COLAB,\n",
        "    torch_dtype=(torch.float16 if \"20b\" in model_name else None),\n",
        ")"
      ],
      "id": "8ce71fd8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0089c69"
      },
      "outputs": [],
      "source": [
        "knowns = KnownsDataset(DATA_DIR)  # Dataset of known facts\n",
        "noise_level = 3 * collect_embedding_std(mt, [k[\"subject\"] for k in knowns])\n",
        "print(f\"Using noise level {noise_level}\")"
      ],
      "id": "b0089c69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c18cfb8"
      },
      "source": [
        "## Tracing a single location\n",
        "\n",
        "The strategy here is to use three interventions, rather than two:\n",
        "\n",
        "1. As before, corrupt a subset of the input.\n",
        "2. As before, restore a subset of the internal hidden states to see\n",
        "   which ones restore the output.\n",
        "3. But now, while doing so, freeze a set of MLP modules when processing\n",
        "   the specific subject token, so that they are stuck in the corrupted\n",
        "   state.  This reveals effect of the hidden states on everything\n",
        "   except for those particular MLP executions.\n",
        "   \n",
        "This three-way intervention is implemented in `trace_with_repatch`"
      ],
      "id": "0c18cfb8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a36b314e"
      },
      "outputs": [],
      "source": [
        "def trace_with_repatch(\n",
        "    model,  # The model\n",
        "    inp,  # A set of inputs\n",
        "    states_to_patch,  # A list of (token index, layername) triples to restore\n",
        "    states_to_unpatch,  # A list of (token index, layername) triples to re-randomize\n",
        "    answers_t,  # Answer probabilities to collect\n",
        "    tokens_to_mix,  # Range of tokens to corrupt (begin, end)\n",
        "    noise=0.1,  # Level of noise to add\n",
        "):\n",
        "    prng = numpy.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n",
        "    patch_spec = defaultdict(list)\n",
        "    for t, l in states_to_patch:\n",
        "        patch_spec[l].append(t)\n",
        "    unpatch_spec = defaultdict(list)\n",
        "    for t, l in states_to_unpatch:\n",
        "        unpatch_spec[l].append(t)\n",
        "\n",
        "    def untuple(x):\n",
        "        return x[0] if isinstance(x, tuple) else x\n",
        "\n",
        "    # Define the model-patching rule.\n",
        "    def patch_rep(x, layer):\n",
        "        if layer == \"transformer.wte\":\n",
        "            # If requested, we corrupt a range of token embeddings on batch items x[1:]\n",
        "            if tokens_to_mix is not None:\n",
        "                b, e = tokens_to_mix\n",
        "                x[1:, b:e] += noise * torch.from_numpy(\n",
        "                    prng.randn(x.shape[0] - 1, e - b, x.shape[2])\n",
        "                ).to(x.device)\n",
        "            return x\n",
        "        if first_pass or (layer not in patch_spec and layer not in unpatch_spec):\n",
        "            return x\n",
        "        # If this layer is in the patch_spec, restore the uncorrupted hidden state\n",
        "        # for selected tokens.\n",
        "        h = untuple(x)\n",
        "        for t in patch_spec.get(layer, []):\n",
        "            h[1:, t] = h[0, t]\n",
        "        for t in unpatch_spec.get(layer, []):\n",
        "            h[1:, t] = untuple(first_pass_trace[layer].output)[1:, t]\n",
        "        return x\n",
        "\n",
        "    # With the patching rules defined, run the patched model in inference.\n",
        "    for first_pass in [True, False] if states_to_unpatch else [False]:\n",
        "        with torch.no_grad(), nethook.TraceDict(\n",
        "            model,\n",
        "            [\"transformer.wte\"] + list(patch_spec.keys()) + list(unpatch_spec.keys()),\n",
        "            edit_output=patch_rep,\n",
        "        ) as td:\n",
        "            outputs_exp = model(**inp)\n",
        "            if first_pass:\n",
        "                first_pass_trace = td\n",
        "\n",
        "    # We report softmax probabilities for the answers_t token predictions of interest.\n",
        "    probs = torch.softmax(outputs_exp.logits[1:, -1, :], dim=1).mean(dim=0)[answers_t]\n",
        "\n",
        "    return probs"
      ],
      "id": "a36b314e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fe52a4a"
      },
      "source": [
        "## Tracing all locations\n",
        "\n",
        "Now we just need to repeat it over all locations, and draw the heatmaps."
      ],
      "id": "6fe52a4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "2d9b5a7c"
      },
      "outputs": [],
      "source": [
        "def calculate_hidden_flow_3(\n",
        "    mt,\n",
        "    prompt,\n",
        "    subject,\n",
        "    token_range=None,\n",
        "    samples=10,\n",
        "    noise=0.1,\n",
        "    window=10,\n",
        "    extra_token=0,\n",
        "    disable_mlp=False,\n",
        "    disable_attn=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs causal tracing over every token/layer combination in the network\n",
        "    and returns a dictionary numerically summarizing the results.\n",
        "    \"\"\"\n",
        "    inp = make_inputs(mt.tokenizer, [prompt] * (samples + 1))\n",
        "    with torch.no_grad():\n",
        "        answer_t, base_score = [d[0] for d in predict_from_input(mt.model, inp)]\n",
        "    [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
        "    e_range = find_token_range(mt.tokenizer, inp[\"input_ids\"][0], subject)\n",
        "    if token_range == \"last_subject\":\n",
        "        token_range = [e_range[1] - 1]\n",
        "    e_range = (e_range[0], e_range[1] + extra_token)\n",
        "    low_score = trace_with_patch(\n",
        "        mt.model, inp, [], answer_t, e_range, noise=noise\n",
        "    ).item()\n",
        "    differences = trace_important_states_3(\n",
        "        mt.model,\n",
        "        mt.num_layers,\n",
        "        inp,\n",
        "        e_range,\n",
        "        answer_t,\n",
        "        noise=noise,\n",
        "        disable_mlp=disable_mlp,\n",
        "        disable_attn=disable_attn,\n",
        "        token_range=token_range,\n",
        "    )\n",
        "    differences = differences.detach().cpu()\n",
        "    return dict(\n",
        "        scores=differences,\n",
        "        low_score=low_score,\n",
        "        high_score=base_score,\n",
        "        input_ids=inp[\"input_ids\"][0],\n",
        "        input_tokens=decode_tokens(mt.tokenizer, inp[\"input_ids\"][0]),\n",
        "        subject_range=e_range,\n",
        "        answer=answer,\n",
        "        window=window,\n",
        "        kind=\"\",\n",
        "    )\n",
        "\n",
        "\n",
        "def trace_important_states_3(\n",
        "    model,\n",
        "    num_layers,\n",
        "    inp,\n",
        "    e_range,\n",
        "    answer_t,\n",
        "    noise=0.1,\n",
        "    disable_mlp=False,\n",
        "    disable_attn=False,\n",
        "    token_range=None,\n",
        "):\n",
        "    ntoks = inp[\"input_ids\"].shape[1]\n",
        "    table = []\n",
        "    zero_mlps = []\n",
        "    if token_range is None:\n",
        "        token_range = range(ntoks)\n",
        "    for tnum in token_range:\n",
        "        zero_mlps = []\n",
        "        if disable_mlp:\n",
        "            zero_mlps = [\n",
        "                (tnum, layername(model, L, \"mlp\")) for L in range(0, num_layers)\n",
        "            ]\n",
        "        if disable_attn:\n",
        "            zero_mlps += [\n",
        "                (tnum, layername(model, L, \"attn\")) for L in range(0, num_layers)\n",
        "            ]\n",
        "        row = []\n",
        "        for layer in range(0, num_layers):\n",
        "            r = trace_with_repatch(\n",
        "                model,\n",
        "                inp,\n",
        "                [(tnum, layername(model, layer))],\n",
        "                zero_mlps,  # states_to_unpatch\n",
        "                answer_t,\n",
        "                tokens_to_mix=e_range,\n",
        "                noise=noise,\n",
        "            )\n",
        "            row.append(r)\n",
        "        table.append(torch.stack(row))\n",
        "    return torch.stack(table)"
      ],
      "id": "2d9b5a7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d559800a"
      },
      "source": [
        "Here is a causal trace with MLP disabled - it looks quite different from normal."
      ],
      "id": "d559800a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "427f3989"
      },
      "outputs": [],
      "source": [
        "prefix = \"Megan Rapinoe plays the sport of\"\n",
        "entity = \"Megan Rapinoe\"\n",
        "\n",
        "no_attn_r = calculate_hidden_flow_3(\n",
        "    mt, prefix, entity, disable_mlp=True, noise=noise_level\n",
        ")\n",
        "plot_trace_heatmap(no_attn_r, title=\"Impact with MLP at last subject token disabled\")\n",
        "ordinary_r = calculate_hidden_flow_3(mt, prefix, entity, noise=noise_level)\n",
        "plot_trace_heatmap(ordinary_r, title=\"Impact with MLP enabled as usual\")"
      ],
      "id": "427f3989"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0362ea3"
      },
      "source": [
        "## Comparing the with-MLP/Attn and without-MLP/Attn traces\n",
        "\n",
        "Plotting on a bar graph makes it easier to see the difference between the causal effects with and without MLP enabled."
      ],
      "id": "a0362ea3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2926aeac"
      },
      "outputs": [],
      "source": [
        "def plot_last_subject(mt, prefix, entity, token_range=\"last_subject\", savepdf=None):\n",
        "    ordinary, no_attn, no_mlp = calculate_last_subject(\n",
        "        mt, prefix, entity, token_range=token_range\n",
        "    )\n",
        "    plot_comparison(ordinary, no_attn, no_mlp, prefix, savepdf=savepdf)\n",
        "\n",
        "\n",
        "def calculate_last_subject(mt, prefix, entity, cache=None, token_range=\"last_subject\"):\n",
        "    def load_from_cache(filename):\n",
        "        try:\n",
        "            dat = numpy.load(f\"{cache}/{filename}\")\n",
        "            return {\n",
        "                k: v\n",
        "                if not isinstance(v, numpy.ndarray)\n",
        "                else str(v)\n",
        "                if v.dtype.type is numpy.str_\n",
        "                else torch.from_numpy(v)\n",
        "                for k, v in dat.items()\n",
        "            }\n",
        "        except FileNotFoundError as e:\n",
        "            return None\n",
        "\n",
        "    no_attn_r = load_from_cache(\"no_attn_r.npz\")\n",
        "    uncached_no_attn_r = no_attn_r is None\n",
        "    no_mlp_r = load_from_cache(\"no_mlp_r.npz\")\n",
        "    uncached_no_mlp_r = no_mlp_r is None\n",
        "    ordinary_r = load_from_cache(\"ordinary.npz\")\n",
        "    uncached_ordinary_r = ordinary_r is None\n",
        "    if uncached_no_attn_r:\n",
        "        no_attn_r = calculate_hidden_flow_3(\n",
        "            mt,\n",
        "            prefix,\n",
        "            entity,\n",
        "            disable_attn=True,\n",
        "            token_range=token_range,\n",
        "            noise=noise_level,\n",
        "        )\n",
        "    if uncached_no_mlp_r:\n",
        "        no_mlp_r = calculate_hidden_flow_3(\n",
        "            mt,\n",
        "            prefix,\n",
        "            entity,\n",
        "            disable_mlp=True,\n",
        "            token_range=token_range,\n",
        "            noise=noise_level,\n",
        "        )\n",
        "    if uncached_ordinary_r:\n",
        "        ordinary_r = calculate_hidden_flow_3(\n",
        "            mt, prefix, entity, token_range=token_range, noise=noise_level\n",
        "        )\n",
        "    if cache is not None:\n",
        "        os.makedirs(cache, exist_ok=True)\n",
        "        for u, r, filename in [\n",
        "            (uncached_no_attn_r, no_attn_r, \"no_attn_r.npz\"),\n",
        "            (uncached_no_mlp_r, no_mlp_r, \"no_mlp_r.npz\"),\n",
        "            (uncached_ordinary_r, ordinary_r, \"ordinary.npz\"),\n",
        "        ]:\n",
        "            if u:\n",
        "                numpy.savez(\n",
        "                    f\"{cache}/{filename}\",\n",
        "                    **{\n",
        "                        k: v.cpu().numpy() if torch.is_tensor(v) else v\n",
        "                        for k, v in r.items()\n",
        "                    },\n",
        "                )\n",
        "    if False:\n",
        "        return (ordinary_r[\"scores\"][0], no_attn_r[\"scores\"][0], no_mlp_r[\"scores\"][0])\n",
        "    return (\n",
        "        ordinary_r[\"scores\"][0] - ordinary_r[\"low_score\"],\n",
        "        no_attn_r[\"scores\"][0] - ordinary_r[\"low_score\"],\n",
        "        no_mlp_r[\"scores\"][0] - ordinary_r[\"low_score\"],\n",
        "    )\n",
        "\n",
        "    # return ordinary_r['scores'][0], no_attn_r['scores'][0]\n",
        "\n",
        "\n",
        "def plot_comparison(ordinary, no_attn, no_mlp, title, savepdf=None):\n",
        "    with plt.rc_context(rc={\"font.family\": \"DejaVu Serif\"}):\n",
        "        import matplotlib.ticker as mtick\n",
        "\n",
        "        fig, ax = plt.subplots(1, figsize=(6, 1.5), dpi=300)\n",
        "        ax.bar(\n",
        "            [i - 0.3 for i in range(len(ordinary))],\n",
        "            ordinary,\n",
        "            width=0.3,\n",
        "            color=\"#7261ab\",\n",
        "            label=\"Impact of single state on P\",\n",
        "        )\n",
        "        ax.bar(\n",
        "            [i for i in range(len(no_attn))],\n",
        "            no_attn,\n",
        "            width=0.3,\n",
        "            color=\"#f3201b\",\n",
        "            label=\"Impact with Attn severed\",\n",
        "        )\n",
        "        ax.bar(\n",
        "            [i + 0.3 for i in range(len(no_mlp))],\n",
        "            no_mlp,\n",
        "            width=0.3,\n",
        "            color=\"#20b020\",\n",
        "            label=\"Impact with MLP severed\",\n",
        "        )\n",
        "        ax.set_title(\n",
        "            title\n",
        "        )  #'Impact of individual hidden state at last subject token with MLP disabled')\n",
        "        ax.set_ylabel(\"Indirect Effect\")\n",
        "        # ax.set_xlabel('Layer at which the single hidden state is restored')\n",
        "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
        "        ax.set_ylim(None, max(0.025, ordinary.max() * 1.05))\n",
        "        ax.legend()\n",
        "        if savepdf:\n",
        "            os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n",
        "            plt.savefig(savepdf, bbox_inches=\"tight\")\n",
        "            plt.close()\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "if False:  # Some representative cases.\n",
        "    plot_last_subject(mt, \"Megan Rapinoe plays the sport of\", \"Megan Rapinoe\")\n",
        "    plot_last_subject(mt, \"The Big Bang Theory premires on\", \"The Big Bang Theory\")\n",
        "    plot_last_subject(mt, \"Germaine Greer's domain of work is\", \"Germaine Greer\")\n",
        "    plot_last_subject(mt, \"Brian de Palma works in the area of\", \"Brian de Palma\")\n",
        "    plot_last_subject(mt, \"The headquarter of Zillow is in downtown\", \"Zillow\")\n",
        "    plot_last_subject(\n",
        "        mt,\n",
        "        \"Mitsubishi Electric started in the 1900s as a small company in\",\n",
        "        \"Mitsubishi\",\n",
        "    )\n",
        "    plot_last_subject(\n",
        "        mt,\n",
        "        \"Mitsubishi Electric started in the 1900s as a small company in\",\n",
        "        \"Mitsubishi Electric\",\n",
        "    )\n",
        "    plot_last_subject(mt, \"Madame de Montesson died in the city of\", \"Madame\")\n",
        "    plot_last_subject(\n",
        "        mt, \"Madame de Montesson died in the city of\", \"Madame de Montesson\"\n",
        "    )\n",
        "    plot_last_subject(mt, \"Edmund Neupert, performing on the\", \"Edmund Neupert\")"
      ],
      "id": "2926aeac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1631379e"
      },
      "outputs": [],
      "source": [
        "plot_last_subject(mt, \"The Space Needle is in the city of\", \"The Space Needle\")"
      ],
      "id": "1631379e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12e3fb70"
      },
      "source": [
        "## Average Indirect Effects\n",
        "\n",
        "Now we average over the first 100 of the factual statements."
      ],
      "id": "12e3fb70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0d802d7"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "knowns = KnownsDataset(DATA_DIR)\n",
        "all_ordinary = []\n",
        "all_no_attn = []\n",
        "all_no_mlp = []\n",
        "for i, knowledge in enumerate(tqdm.tqdm(knowns[:100])):\n",
        "    # plot_all_flow(mt, knowledge['prompt'], knowledge['subject'])\n",
        "    ordinary, no_attn, no_mlp = calculate_last_subject(\n",
        "        mt,\n",
        "        knowledge[\"prompt\"],\n",
        "        knowledge[\"subject\"],\n",
        "        cache=f\"results/ct_disable_attn/case_{i}\",\n",
        "    )\n",
        "    all_ordinary.append(ordinary)\n",
        "    all_no_attn.append(no_attn)\n",
        "    all_no_mlp.append(no_mlp)\n",
        "title = \"Causal effect of states at the early site with Attn or MLP modules severed\"\n",
        "\n",
        "avg_ordinary = torch.stack(all_ordinary).mean(dim=0)\n",
        "avg_no_attn = torch.stack(all_no_attn).mean(dim=0)\n",
        "avg_no_mlp = torch.stack(all_no_mlp).mean(dim=0)\n",
        "import matplotlib.ticker as mtick\n",
        "\n",
        "with plt.rc_context(rc={\"font.family\": \"DejaVu Serif\"}):\n",
        "    fig, ax = plt.subplots(1, figsize=(6, 2.1), dpi=300)\n",
        "    ax.bar(\n",
        "        [i - 0.3 for i in range(48)],\n",
        "        avg_ordinary,\n",
        "        width=0.3,\n",
        "        color=\"#7261ab\",\n",
        "        label=\"Effect of single state on P\",\n",
        "    )\n",
        "    ax.bar(\n",
        "        [i for i in range(48)],\n",
        "        avg_no_attn,\n",
        "        width=0.3,\n",
        "        color=\"#f3201b\",\n",
        "        label=\"Effect with Attn severed\",\n",
        "    )\n",
        "    ax.bar(\n",
        "        [i + 0.3 for i in range(48)],\n",
        "        avg_no_mlp,\n",
        "        width=0.3,\n",
        "        color=\"#20b020\",\n",
        "        label=\"Effect with MLP severed\",\n",
        "    )\n",
        "    ax.set_title(\n",
        "        title\n",
        "    )  #'Impact of individual hidden state at last subject token with MLP disabled')\n",
        "    ax.set_ylabel(\"Average Indirect Effect\")\n",
        "    ax.set_xlabel(\"Layer at which the single hidden state is restored\")\n",
        "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
        "    ax.set_ylim(None, max(0.025, 0.105))\n",
        "\n",
        "    ax.legend(frameon=False)\n",
        "fig.savefig(\"causal-trace-no-attn-mlp.pdf\", bbox_inches=\"tight\")\n",
        "print([d[20] - d[10] for d in [avg_ordinary, avg_no_attn, avg_no_mlp]])\n",
        "print(avg_ordinary[15], avg_no_attn[15], avg_no_mlp[15])"
      ],
      "id": "e0d802d7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}